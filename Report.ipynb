{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1 style=\"text-align:center\">ASAT </h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h2 style=\"text-align:center;margin-bottom:50px\">Aeronautics Recruitment: Software Development\r\n",
    "Subsystem\r\n",
    "</h2>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"margin-bottom: 100px\">\r\n",
    "    <h3>Autopilot Overview</h3>\r\n",
    "    <p>For a UAV to be able to function autonomously, an interactive understanding of its environment is essential. To achieve that, the vehicle needs to integrate a series of sensors that will provide information on its location, orientation, speed, altitude, etc. Such sensors are a 3-axis gyroscope, a GPS (Global Positioning System), an accelerometer, an altimeter, and an airspeed sensor. Due to the objective of project Phoenix (fire detection), a thermal and a regular camera are required. For the UAV to be able to communicate and detect other aircraft, it can use ADS-B technology (Automatic Dependent Surveillance-Broadcast). The hardware mentioned above should comply with the pixhawk open standards to avoid unnecessary incompatibilities</p>\r\n",
    "    <p>Apart from the sensor's data, the autopilot needs additional input from the user to plan its route accordingly. The user shall specify the destination, flight altitude, and speed of the aircraft. There should also be an option to add waypoints. With the purpose of fire detection, it should include a mode that allows the user to select an area for scanning, letting the UAV roam above it.</p>\r\n",
    "    <p>To process this data, we are to use open source autopilot software, such as Px4 and ArduPilot. Both of these software use MAVlink (Micro Air Vehicle communication protocol) to communicate with the user/control station. To set the input parameters (route, waypoints) and monitor the vehicle, we can use a MAVlink based control station, like QGroundControl.</p>\r\n",
    "    <p> In case the user chooses an impossible route, an error should appear asking them to select different parameters or manually operate the UAV. If the error occurs while on air (obstacle, weather conditions, etc) the autopilot should re-plan its path using a path optimizing algorithm, or -in extreme conditions- land to the nearest safe point. Safe points could be pre-determined by the user for extra security.</p>\r\n",
    "    <p>Finally, to ensure the correct operation of the system, we can run simulations in software such as AirSim (SITL), Gazebo, jMAVSim.</p>\r\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div>\r\n",
    "    <h3>Dijkstra's Algorithm</h3>\r\n",
    "    <p>Dijkstra's Algorithm is an algorithm for finding the shortest paths between nodes in a graph. In this implementation, we will be using a node as our base node (source) and computing the shortest path from the source to every other node.</p>\r\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>To use certain math operations we import NumPy </p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>We are given 5 nodes:<br>\r\n",
    "Node 1 = connection[0] = [0,1,0,3,10]<br>\r\n",
    "Node 2 = connction[1] = [1, 0, 5, 0, 0]<br>\r\n",
    ".<br>.<br>.<br>\r\n",
    "Where each number inside the lists represents the weight (or difficulty) of the connection between the nodes.<br>\r\n",
    "For exampe: node 1 is not connected to itself (0), is connected to node 2 with a weight of 1 (easy to reach), is not connected to node 3, is connected to node 4 with a weight of 3 (slightly more difficult to reach), and is connected to node 5 with a weight of 10 (difficult to reach). <br>\r\n",
    "Based on this data we create the following list (you can think of it as an array, though in reality it is a python list):</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "connections = [\r\n",
    "    [0, 1, 0, 3, 10],\r\n",
    "    [1, 0, 5, 0, 0],\r\n",
    "    [0, 5, 0, 2, 1],\r\n",
    "    [3, 0, 2, 0, 6],\r\n",
    "    [10, 0, 1, 6, 0]\r\n",
    "    ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>We know that our source node is node 1, so we use its index [0] to specify its value :</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "baseNode=0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>\r\n",
    "    We will be tracking the total path weight of each node (from the source node) in a list named weight_sum wich will contain as many elements as the nodes (5).<br>\r\n",
    "    We will also create a list named unvisited which will contain the indexes (of the weight_sum list) of the nodes that have not yet been completed.,<br>\r\n",
    "    The minimum could be any positive number, we just want to initialize it.<br>\r\n",
    "    Finally, we set the weight of the basenode to 0 (we are already there so the path is 0)\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "weight_sum=[]\r\n",
    "unvisited=[]\r\n",
    "minimum=0\r\n",
    "for i in range(0, len(connections)):\r\n",
    "    weight_sum.append(np.infty)\r\n",
    "    unvisited.append(i)\r\n",
    "weight_sum[baseNode]=0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>The following piece of code finds the minimum weight_sum out of all the unvisited indexes, and visits that node to examine its neighboring nodes. <br>If the weight of the connection is 0, it ignores it (they are not connected). For non zero values it checks if this new path is shorter than the already registered and it updates the value to the minimum.<br> It removes the visited node from the unvisited list and resets the minimum to a large enough value that will allow the correct function of the loop.</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "while len(unvisited) != 0:\r\n",
    "    for i in unvisited:\r\n",
    "        if minimum>=weight_sum[i]:\r\n",
    "            minimum=weight_sum[i]\r\n",
    "    i=weight_sum.index(minimum)\r\n",
    "    unvisited.remove(i)\r\n",
    "    for j in range(0, len(connections)):\r\n",
    "        if connections[i][j]!=0:\r\n",
    "            weight_sum[j]=min(weight_sum[j], weight_sum[i]+connections[i][j])\r\n",
    "    minimum=max(weight_sum)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>The only thing left is to print the minimum weight sums for each node to the console:</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "print(\"==========================================\")\r\n",
    "print(\"Starting from node \" + str(baseNode+1) + \" :\")\r\n",
    "print(\"Minimum distance to other nodes:\\n\")\r\n",
    "print(\"Node \\t\\t Weight-Sum/Distance\\n------------------------------------------\")\r\n",
    "\r\n",
    "for j in range(0, len(connections)):\r\n",
    "    print(j+1, \"\\t\\t\", weight_sum[j])\r\n",
    "\r\n",
    "print(\"\\n==========================================\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==========================================\n",
      "Starting from node 1 :\n",
      "Minimum distance to other nodes:\n",
      "\n",
      "Node \t\t Weight-Sum/Distance\n",
      "------------------------------------------\n",
      "1 \t\t 0\n",
      "2 \t\t 1\n",
      "3 \t\t 5\n",
      "4 \t\t 3\n",
      "5 \t\t 6\n",
      "\n",
      "==========================================\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3 style=\"margin-top:100px\">\r\n",
    "    Digit Classification\r\n",
    "</h3>\r\n",
    "<p>\r\n",
    "    In the following pieces of code we will create and train a deep learning model to recognize and classify handwritten digits. To do that we will be using tensorflow and numpy, which include functions that simplify the process:\r\n",
    "</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>To train our model we will be using the MNIST dataset; a collection of 60,000 training images of 28x28 px and 10,000 testing images of digits 0 to 9. The data are in a form of a tuple (x,y) where x is the image and y is the label. <br>For example if x is an image of the number 5 then y is 5. To load the data we type: </p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "mnist=tf.keras.datasets.mnist"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>We now create two tuple that contein the data, one for training the model and one for testing:</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "(training_data,training_labels),(test_data,test_labels)=mnist.load_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p> Because the x value represents the brightness of the pixel (the images are black and white), its maximum value would be 255. We devide by that value in order to get a float between 0 and 1 (1 being white and 0 black), representing the percentage of brightness.</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "training_data=training_data/255\r\n",
    "test_data=test_data/255"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Now it's time to create the model. We will create three layers: The input layer, one hidden and the output layer.<br>\r\n",
    " For the input we used .Flatten in order to use each pixel as one neuron (input) resulting to 28x28=784 input neurons.<br>\r\n",
    " On the hidden layer we used 120 neurons (that number is relatively flexible) and the activation function relu, because we needed an output between 0 and 1. We could have used sigmoid or any other similar function, but relu seemed to have a higher accuracy.<br>\r\n",
    " Finally for the output layer, we need 10 neurons, one for each digit. We chose the function softmax because it ranges from 0 to 1, while at the same time the sum of all outputs is 1, corrisponding to 100%. This is convinient, because we are checking how likely it is that the image is each digit, and then selecting the digit with the highest probability.\r\n",
    " </p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "model= tf.keras.Sequential([\r\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28)),\r\n",
    "    tf.keras.layers.Dense(120,activation=tf.nn.relu),\r\n",
    "    tf.keras.layers.Dense(10,activation=tf.nn.softmax)\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>We set the optimizer to Adam and the loss function to categorical crossentropy. </p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "model.compile(\r\n",
    "    optimizer=tf.optimizers.Adam(),\r\n",
    "    loss='sparse_categorical_crossentropy',\r\n",
    "    metrics='accuracy'\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Our model has not recieved any data at this point. To train it we use .fit and input the training data and labels, as well as the number of epochs. One epoch is one \"pass\" of the entire dataset through the model.</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "model.fit(training_data,training_labels,epochs=20)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0138 - accuracy: 0.9956\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0107 - accuracy: 0.9969\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0109 - accuracy: 0.9964\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0086 - accuracy: 0.9973\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0085 - accuracy: 0.9973\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0051 - accuracy: 0.9987\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0073 - accuracy: 0.9976\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0065 - accuracy: 0.9978\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0061 - accuracy: 0.9981\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0063 - accuracy: 0.9980\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0049 - accuracy: 0.9984\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0041 - accuracy: 0.9988\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0060 - accuracy: 0.9979\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0046 - accuracy: 0.9985\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0043 - accuracy: 0.9986\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0039 - accuracy: 0.9988\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0054 - accuracy: 0.9981\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0036 - accuracy: 0.9988\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0041 - accuracy: 0.9987\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0041 - accuracy: 0.9986\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27c7f8d28e0>"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>To check how well our model works on data it hasn't been trained on we use .evaluate and input the test data.</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "model.evaluate(test_data,test_labels)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0884 - accuracy: 0.9775\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.08837443590164185, 0.9775000214576721]"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<p>Finally, if we needed to make predictions about images of which we do not have the labels we would use .predict as follows</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "predictions=model.predict(test_data)\r\n",
    "np.set_printoptions(suppress=True)\r\n",
    "print(test_labels[0])\r\n",
    "print(predictions[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "b8c4aa714b434934377800ec08a936821a0c59d2fd3ecee2ac492715905cb910"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}